<!DOCTYPE html>
<html>

<head>
    <meta charset="utf-8">
    <meta name="description" content="VADAR">
    <meta name="keywords" content="VADAR, Omni3D-Bench, Program Synthesis">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <title>No Labels, No Problem: Training Visual Reasoners with Multimodal Verifiers</title>


    <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro" rel="stylesheet">

    <link rel="stylesheet" href="./static/css/bulma.min.css">
    <link rel="stylesheet" href="./static/css/bulma-carousel.min.css">
    <link rel="stylesheet" href="./static/css/bulma-slider.min.css">
    <link rel="stylesheet" href="./static/css/fontawesome.all.min.css">
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
    <link rel="stylesheet" href="./static/css/index.css">
    <link rel="icon" href="./static/images/vader_heart.png">

    <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
    <script src="https://cdn.plot.ly/plotly-2.9.0.min.js"></script>
    <script src="https://cdn.plot.ly/plotly-2.9.0.min.js"></script>
    <script defer src="./static/js/fontawesome.all.min.js"></script>
    <script src="./static/js/bulma-carousel.min.js"></script>
    <script src="./static/js/bulma-slider.min.js"></script>
    <script src="./static/js/index.js"></script>
</head>

<body>
    <section class="hero">
        <div class="hero-body">
            <div class="container is-max-desktop">
                <div class="columns is-centered">
                    <div class="column has-text-centered">
                        <div class="valor-title">
                            <img src="./static/images/valor_logo.png" alt="VALOR logo" class="valor-logo">
                            <h1 class="title is-1 publication-title">VALOR</h1>
                        </div>
                        <h1 class="title is-1 publication-title">No Labels, No Problem: Training Visual Reasoners with
                            Multimodal Verifiers</h1>
                        <div class="is-size-5 publication-authors">
                            <span class="author-block">
                                <a href="https://damianomarsili.github.io/" target="_blank"
                                    rel="noopener noreferrer">Damiano Marsili</a>,</span>
                            <span class="author-block">
                                <a href="https://georgiagkioxari.com/" target="_blank" rel="noopener noreferrer">Georgia
                                    Gkioxari</a></span>
                        </div>
                        <div class="is-size-5 publication-authors">
                            <span class="author-block">Caltech</span>
                        </div>
                        <!-- DAMI up to here. -->
                        <div class="column has-text-centered">
                            <div class="publication-links">
                                <!-- PDF Link. -->
                                <span class="link-block">
                                    <a href="" class="external-link button is-normal is-rounded is-dark">
                                        <span class="icon">
                                            <i class="fas fa-file-pdf"></i>
                                        </span>
                                        <span>Paper</span>
                                    </a>
                                </span>
                                <span class="link-block">
                                    <a href="" class="external-link button is-normal is-rounded is-dark">
                                        <span class="icon">
                                            <i class="ai ai-arxiv"></i>
                                        </span>
                                        <span>arXiv</span>
                                    </a>
                                </span>
                                <!-- Code Link. -->
                                <span class="link-block">
                                    <a href="https://github.com/damianomarsili/VALOR"
                                        class="external-link button is-normal is-rounded is-dark">
                                        <span class="icon">
                                            <i class="fab fa-github"></i>
                                        </span>
                                        <span>Code</span>
                                    </a>
                                </span>
                            </div>
                        </div>


                    </div>
                </div>
            </div>
        </div>
    </section>

    <section class="section">
        <div class="container is-max-desktop">
            <!-- Abstract. -->
            <div class="columns is-centered has-text-centered">
                <div class="content has-text-centered">
                    <p>
                        <b>tl;dr</b>
                    </p>
                </div>
            </div>
        </div>
        <!--/ Abstract. -->
        </div>
    </section>

    <section class="section">
        <div class="container is-max-desktop">
            <!-- Abstract. -->
            <div class="columns is-centered has-text-centered">
                <div class="column ">
                    <div class="content has-text-justified">
                        <h4>Progress in visual reasoning has been slower than text-based reasoning.</h4>
                        <figure class="image is-fullwidth" style="margin: 1rem 0;">
                            <img src="./static/images/4o_vs_gpt5_combined.png" alt="The improvements on text-based benchmarks have been far greater than visual
                                reasoning benchmarks when comparing GPT-5-mini to GPT4o.">
                            <!-- optional caption -->
                            <figcaption class="has-text-centered is-size-7" style="margin-top: 0.5rem;">
                                Figure 1: The improvements on text-based benchmarks have been far greater than visual
                                reasoning benchmarks when comparing GPT-5-mini to GPT4o.
                            </figcaption>
                        </figure>

                        <p>
                            The progress of LLMs in text-based reasoning in 2024-2025 has been remarkable. In the course
                            of a year, we saw LLMs go from general-purpose chatbots to solving competition-level math
                            problems, writing sophisticated code in large repositories, and answering expert-level
                            questions in the sciences. This rate of progress is apparent when comparing benchmark
                            results from different model releases. For example, GPT4o achieved just 15.0% on the math
                            problems in AIME 2025, but OpenAI's later model release of GPT-5-mini achieves a staggering
                            94.0%. Similar results are seen on software engineering benchmark LiveCodeBench (23.0% to
                            85.0%) and science VQA benchmark GPQA (40.2% to 82.3%).
                        </p>
                        <p>
                            Unfortunately, progress in visual reasoning has been significantly slower. Comparing the
                            same two models, the improvements across benchmarks are much more modest: 73.7% to 81.0% on
                            BLINK, 80.9% to 84.4% on VSR, and 84.7% to 84.3% on CountBenchQA — a slight decrease. On
                            the harder 3D spatial reasoning benchmark Omni3D-Bench, accuracy is still quite
                            low — GPT-5-mini reaches 40.9%, only 5.9% above GPT-4o. This discrepancy is not
                            surprising, visual reasoning is a challenging task. It requires precise object
                            grounding and understanding complex spatial relationships, both of which remain challenging
                            for current models.
                        </p>
                        <h4>Improving Visual Reasoning</h4>

                        <p>
                            TODO: introduce two camps of works
                        </p>

                        <h4>The VALOR Framework</h4>
                        <figure class="image is-fullwidth" style="margin: 1rem 0;">
                            <img src="./static/images/valor.png" alt="VALOR framework.">
                            <!-- optional caption -->
                            <figcaption class="has-text-centered is-size-7" style="margin-top: 0.5rem;">
                                Figure 2: VALOR is a training framework for visual reasoning, tackling spatial reasoning
                                in both 2D and 3D. During training, LLM verifiers are used to improve reasoning via RL
                                while VLM verifiers serve as critics to tune vision grounding models
                                via SFT.
                            </figcaption>
                        </figure>

                        <p>
                            We tackle spatial reasoning from images by combining LLM-powered reasoning with specialized
                            tool use. VALOR employs an LLM to generate plans and executable programs and invokes vision
                            specialists for execution. Both the reasoning and the vision grounding model are tuned for
                            the task via a label-free training paradigm. This is achieved by leveraging multimodal
                            verifiers that critique model outputs. Their feedback serves as a learning signal to improve
                            both components, the LLM responsible for logic and the vision specialists responsible for
                            grounding. Figure 2 shows an overview of our approach.
                        </p>

                        <p>
                            <strong>Plan and Code Generation.</strong> Given a query, the LLM generates a natural
                            language plan followed by a corresponding program in Python. Available to the LLM are the
                            APIs of three function calls:
                        </p>

                        <ul>
                            <li>
                                <code>GD_DETECT</code>, returns the bounding box of all object instances specified by
                                the noun description —
                                e.g., <code>GD_DETECT("CAR")</code>
                            </li>
                            <li>
                                <code>DEPTH</code>, returns the depth of a pixel in the image —
                                <code>DEPTH(IMAGE, X, Y)</code>
                            </li>
                            <li>
                                <code>VQA</code>, returns an object’s attribute (e.g., color) from the input image crop
                                around the object —
                                e.g., <code>VQA(IMAGE_CROP, "WHAT IS THE COLOR OF THE OBJECT IN THE IMAGE?")</code>
                            </li>
                        </ul>

                        <p>
                            <strong>Vision Specialists.</strong> VALOR employs three vision specialist models:
                            GroundingDINO for object localization (<code>GD DETECT</code>), MoGe2 for pointwise metric
                            depth estimation (<code>DEPTH</code>), and GPT-5-mini for VQA
                            with an image cropped around an object bounding box (<code>VQA</code>).
                        </p>

                        <h4>Improving Reasoning with LLM Verifiers</h4>
                        <figure class="image is-fullwidth" style="margin: 1rem 0;">
                            <video id="teaser" autoplay muted loop playsinline height="80%">
                                <source src="./static/videos/llm_verifier.mp4" type="video/mp4">
                            </video>
                            <!-- optional caption -->
                            <figcaption class="has-text-centered is-size-7" style="margin-top: 0.5rem;">
                                TODO: fig caption
                            </figcaption>
                        </figure>

                        <p>
                            TODO: introduce VALOR framework.
                        </p>

                        <h4>Improving Visual Grounding with VLM Verifiers</h4>
                        <figure class="image is-fullwidth" style="margin: 1rem 0;">
                            <img src="./static/images/verifiers_preview.png" alt="TODO..">
                            <!-- optional caption -->
                            <figcaption class="has-text-centered is-size-7" style="margin-top: 0.5rem;">
                                TODO: replace with video.
                            </figcaption>
                        </figure>

                        <p>
                            TODO: introduce VALOR framework.
                        </p>

                        <h4>Can trained models ever outperform the verifiers?</h4>
                        <figure class="image is-fullwidth" style="margin: 1rem 0;">
                            <img src="./static/images/valor_vs_gpt5.png"
                                alt="Despite being a good verifier for visual grounding, GPT5 struggles with visual grounding.">
                            <!-- optional caption -->
                            <figcaption class="has-text-centered is-size-7" style="margin-top: 0.5rem;">
                                Figure: Despite being a powerful VLM verifier for visual grounding, GPT-5-mini struggles
                                with generating boxes for object grounding.
                            </figcaption>
                        </figure>

                        <p>
                            A natural question to ask is whether VALOR can ever outperform the veriifers it uses during
                            training. To this end, we first note that VALOR uses multimodal verifiers to <em>select</em>
                            and <em>critique</em> data, not <em>generate</em> it. Thus, VALOR is not bound by the
                            <em>generation</em> abilities of an LLM/VLM, but rather it's <em>verification</em> ability.
                            This distinction is important as we find there are tasks where VLMs are better verifiers
                            than generators. For a concrete example, in VALOR, we use GPT-5-mini as our VLM verifier for
                            improving the visual grounding module. Although highly effective at evaluating object
                            detections, we observe that it often struggles generating bounding boxes itself. In the
                            figure above, we find that GPT-5-mini frequently outputs misaligned or overly large boxes,
                            failing to localize objects that VALOR (trained with GPT-5-mini as a verifier) correctly
                            detects. We find that a VLM canprovide reliable binary judgements about correctness even
                            when its own grounding predictions are imperfect.
                        </p>

                        <p>
                            TODO: introduce experimental setup/all use tools etc.
                        </p>

                        <h4>How do open-source models perform?</h4>
                        <figure class="image is-fullwidth" style="margin: 1rem 0;">
                            <img src="./static/images/os_bar_plot.png" alt="">
                            <!-- optional caption -->
                            <figcaption class="has-text-centered is-size-7" style="margin-top: 0.5rem;">
                                Figure: Open-source language-only models generate Python programs with
                                access to an API of vision specialist models. We execute the programs and report
                                accuracy.
                            </figcaption>
                        </figure>

                        <p>
                            We evaluated a series of open-source models across a wide range of spatial reasoning
                            benchmarks. Each model was used language-only, and was prompted to generate Python programs
                            that could invoke an API of vision specialist models (detection, depth estimation, VQA).
                            Among the open-source models we evaluated (Llama3.2-11B, Gemma3-12B,and Qwen3-8B), Qwen3
                            consistently performed the best. We found that despite using the
                            instruction-tuned variants, Gemma3 and Llama3.2 routinely ignored our system prompts. For
                            example, both models would frequently overwrite the input image path, define "placeholder"
                            values, or argue the query was impossible and refuse to answer altogether. In contrast,
                            Qwen3 consistently produced reasonable programs, but incorrectly handled nuanced details in
                            the query and failed to use tools effectively. We felt these were issues that could be
                            addressed via post-training, so decided to build on the capable Qwen3 model for VALOR.
                        </p>

                        <h4>Qwen3 vs VALOR-RL: Training with verifiers improves model reasoning.</h4>
                        <figure class="image is-fullwidth" style="margin: 1rem 0;">
                            <img src="./static/images/os_valor_rl.png"
                                alt="Training with verifiers improves model reasoning">
                            <!-- optional caption -->
                            <figcaption class="has-text-centered is-size-7" style="margin-top: 0.5rem;">
                                Figure: Verifier-guided training improves model reasoning and tool use, particularly on
                                reasoning-heavy benchmark Omni3D-Bench.
                            </figcaption>
                        </figure>
                        <p>
                            We compare VALOR-RL with Qwen3 to isolate the impact of verifier-improved reasoning.
                            VALOR-RL uses a verifier-trained Qwen3 model with the same vision specialist models. Thus
                            any improvements from Qwen3 to VALOR-RL stem from our LLM-verifier guided training. VALOR-RL
                            shows gains over Qwen3: +3.4% on BLINK, +2.1% on VSR, and +1.3% on RoboSpatial. Most
                            notably, VALOR-RL greatly improves on Omni3D-Bench (+6.4%), our most reasoning-intensive
                            benchmark. On counting tasks TallyQA and CountBenchQA, reasoning is less critical, and
                            VALOR-RL matches Qwen3.
                        </p>
                        <h4>VALOR-RL vs VALOR: Training with verifiers improves visual grounding.</h4>
                        <figure class="image is-fullwidth" style="margin: 1rem 0;">
                            <img src="./static/images/valor_vs_valor_rl.png"
                                alt="Training with verifiers improves visual grounding">
                            <!-- optional caption -->
                            <figcaption class="has-text-centered is-size-7" style="margin-top: 0.5rem;">
                                Figure: Verifier-guided training improves visual grounding, yielding improved
                                performance across benchmarks.
                            </figcaption>
                        </figure>
                        <p>
                            In the above plot we compare VALOR, our final method, to VALOR-RL. The two variants execute
                            identical programs, though VALOR uses the verifier-improved visual grounding module. VALOR
                            yields strong gains across the board, particularly on grounding-focused benchmarks: +8.3%
                            on CountBenchQA, +7.7% on RoboSpatial, and +5.3% on VSR. Improvements on Omni3D-Bench are
                            smaller, as complex queries make reasoning the main challenge for smaller LLMs. Notably,
                            improving visual grounding for spatial reasoning does not harm general object detection; our
                            training slightly boosts performance on the COCO validation set: 48.4% to
                            48.7% mAP.
                        </p>
                        <h4>VALOR Outputs</h4>
                        <p>
                            TODO: Video of examples.
                        </p>
                        <h4>Conclusion</h4>
                        <p>
                            We introduce VALOR, an annotation-free training paradigm for visual reasoning that leverages
                            multimodal verifiers to improve LLM reasoning and visual grounding. We find that VLMs/LLMs
                            are increasingly capable verifiers, not merely generators. In fact, we find there are tasks
                            where they are excellent verifiers but not great generators (e.g. object detection). This
                            suggests an alternative method to improving reasoning in the visual domain – leveraging the
                            multimodal verification capabilities of these models to enable training in domains where
                            ground truth is unavailable.
                        </p>

                        <strong>Recommended Reading</strong>
                        <ol>
                            <li><a href="https://visually-grounded-rl.github.io/">Visually Grounded Reinforcement
                                    Learning</a></li>
                            <li><a href="https://grounded-reasoning.github.io/">Teaching MLLMs to Think with Images</a>
                            </li>
                            <li><a href="https://openai.com/index/thinking-with-images/">Thinking with Images</a></li>
                            <li><a href="https://glab-caltech.github.io/vadar/">Visual Agentic AI for Spatial Reasoning
                                    with a Dynamic API</a></li>
                        </ol>
                        <strong>Acknowledgements</strong>
                        <p>
                            We would like to thank ...
                        </p>
                    </div>
                </div>
            </div>
            <!--/ Abstract. -->
        </div>
    </section>


    <section class="section" id="BibTeX">
        <div class="container is-max-desktop content">
            <h2 class="title">BibTeX</h2>
            <pre>
<code>TODO.</code></pre>
        </div>
    </section>

    <div id="modal" class="modal">
        <div class="modal-background"></div>

        <div class="modal-content">
            <div id='plot-loading-div' class="box">
                Loading...
            </div>
            <div id='plot-div'></div>
        </div>

        <button class="modal-close is-large" aria-label="close"></button>
    </div>

</body>

</html>