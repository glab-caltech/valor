<!DOCTYPE html>
<html>

<head>
    <meta charset="utf-8">
    <meta name="description" content="VADAR">
    <meta name="keywords" content="VADAR, Omni3D-Bench, Program Synthesis">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <title>No Labels, No Problem: Training Visual Reasoners with Multimodal Verifiers</title>


    <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro" rel="stylesheet">

    <link rel="stylesheet" href="./static/css/bulma.min.css">
    <link rel="stylesheet" href="./static/css/bulma-carousel.min.css">
    <link rel="stylesheet" href="./static/css/bulma-slider.min.css">
    <link rel="stylesheet" href="./static/css/fontawesome.all.min.css">
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
    <link rel="stylesheet" href="./static/css/index.css">
    <link rel="icon" href="./static/images/vader_heart.png">

    <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
    <script src="https://cdn.plot.ly/plotly-2.9.0.min.js"></script>
    <script src="https://cdn.plot.ly/plotly-2.9.0.min.js"></script>
    <script defer src="./static/js/fontawesome.all.min.js"></script>
    <script src="./static/js/bulma-carousel.min.js"></script>
    <script src="./static/js/bulma-slider.min.js"></script>
    <script src="./static/js/index.js"></script>

    <!-- MathJax -->
    <script>
        window.MathJax = {
            tex: {
                inlineMath: [['\\(', '\\)'], ['$', '$']],
                displayMath: [['\\[', '\\]'], ['$$', '$$']]
            }
        };
    </script>
    <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js">
    </script>
</head>

<body>
    <section class="hero">
        <div class="hero-body">
            <div class="container is-max-desktop">
                <div class="columns is-centered">
                    <div class="column has-text-centered">
                        <div class="valor-title">
                            <img src="./static/images/valor_logo.png" alt="VALOR logo" class="valor-logo">
                            <h1 class="title is-1 publication-title">VALOR</h1>
                        </div>
                        <h1 class="title is-1 publication-title">No Labels, No Problem: Training Visual Reasoners with
                            Multimodal Verifiers</h1>
                        <div class="is-size-5 publication-authors">
                            <span class="author-block">
                                <a href="https://damianomarsili.github.io/" target="_blank"
                                    rel="noopener noreferrer">Damiano Marsili</a>,</span>
                            <span class="author-block">
                                <a href="https://georgiagkioxari.com/" target="_blank" rel="noopener noreferrer">Georgia
                                    Gkioxari</a></span>
                        </div>
                        <div class="is-size-5 publication-authors">
                            <span class="author-block">Caltech</span>
                        </div>
                        <!-- DAMI up to here. -->
                        <div class="column has-text-centered">
                            <div class="publication-links">
                                <!-- PDF Link. -->
                                <span class="link-block">
                                    <a href="" class="external-link button is-normal is-rounded is-dark">
                                        <span class="icon">
                                            <i class="fas fa-file-pdf"></i>
                                        </span>
                                        <span>Paper</span>
                                    </a>
                                </span>
                                <span class="link-block">
                                    <a href="" class="external-link button is-normal is-rounded is-dark">
                                        <span class="icon">
                                            <i class="ai ai-arxiv"></i>
                                        </span>
                                        <span>arXiv</span>
                                    </a>
                                </span>
                                <!-- Code Link. -->
                                <span class="link-block">
                                    <a href="https://github.com/damianomarsili/VALOR"
                                        class="external-link button is-normal is-rounded is-dark">
                                        <span class="icon">
                                            <i class="fab fa-github"></i>
                                        </span>
                                        <span>Code</span>
                                    </a>
                                </span>
                            </div>
                        </div>


                    </div>
                </div>
            </div>
        </div>
    </section>

    <section class="section">
        <div class="container is-max-desktop">
            <!-- Abstract. -->
            <div class="columns is-centered has-text-centered">
                <div class="content has-text-centered">
                    <p>
                        <b>tl;dr:</b> We introduce VALOR, an annotation-free framework that boosts both visual reasoning
                        and
                        grounding by training with AI verifiers instead of human labels. A language model verifier
                        improves reasoning through reinforcement learning, while a vision-language verifier enhances
                        grounding via automatic hard-negative mining. The result is a stronger visual reasoning system
                        that outperforms open-source and proprietary models across a suite of visual reasoning
                        benchmarks.
                    </p>
                </div>
            </div>
        </div>
        <!--/ Abstract. -->
        </div>
    </section>

    <section class="section">
        <div class="container is-max-desktop">
            <!-- Abstract. -->
            <div class="columns is-centered has-text-centered">
                <div class="column ">
                    <div class="content has-text-justified">
                        <h4>Progress in visual reasoning has been slower than text-based reasoning.</h4>
                        <figure class="image is-fullwidth" style="margin: 1rem 0;">
                            <img src="./static/images/4o_vs_gpt5_combined.png" alt="The improvements on text-based benchmarks have been far greater than visual
                                reasoning benchmarks when comparing GPT-5-mini to GPT4o.">
                            <!-- optional caption -->
                            <figcaption class="has-text-centered is-size-7" style="margin-top: 0.5rem;">
                                Figure 1: The improvements on text-based benchmarks have been far greater than visual
                                reasoning benchmarks when comparing GPT-5-mini to GPT4o.
                            </figcaption>
                        </figure>

                        <p>
                            The progress of LLMs in text-based reasoning in 2024-2025 has been remarkable. In the course
                            of a year, we saw LLMs go from general-purpose chatbots to solving competition-level math
                            problems, writing sophisticated code in large repositories, and answering expert-level
                            questions in the sciences. This rate of progress is apparent when comparing benchmark
                            results from different model releases. For example, GPT4o achieved just 15.0% on the math
                            problems in AIME 2025, but OpenAI's later model release of GPT-5-mini achieves a staggering
                            94.0%. Similar results are seen on software engineering benchmark LiveCodeBench and science
                            VQA benchmark GPQA.
                        </p>
                        <p>
                            Unfortunately, progress in visual reasoning has been significantly slower. Comparing the
                            same two models, the improvements across benchmarks are much more modest: +7.3% on BLINK,
                            +3.5% on VSR, and a slight decrease on CountBenchQA. On the harder 3D spatial reasoning
                            benchmark Omni3D-Bench, accuracy is still quite low: GPT-5-mini reaches 40.9%, only 5.9%
                            above GPT-4o. This discrepancy is not surprising, visual reasoning is a challenging task. It
                            requires precise object grounding and understanding complex spatial relationships, both of
                            which remain challenging for current models.
                        </p>

                        <figure class="image is-fullwidth" style="margin: 1rem 0;">
                            <img src="./static/images/gpt_5_o3db.png" alt="GPT-5 failing on Omni3D-Bench.">
                            <!-- optional caption -->
                            <figcaption class="has-text-centered is-size-7" style="margin-top: 0.5rem;">
                                Figure 2: GPT-5-Thinking on Omni3D-Bench. GPT-5-Thinking ignores real-world 3D object
                                sizes and considers only pixel-wise dimensions, leading to an incorrect answer. We
                                highlight erroneous reasoning in red.
                            </figcaption>
                        </figure>


                        <p>
                            Methods to improve visual reasoning broadly fall into two categories. The first
                            integrates grounding with language reasoning, where vision-language models
                            (VLMs) generate chain-of-thought explanations in text. Examples include Thinking with Images
                            <a href="#thinking-with-images">[1]</a>, GRIT <a href="#grit">[2]</a>, and Visually Grounded
                            RL <a href="#vigorl">[3]</a>. These methods can handle simple
                            spatial relations, but suffer from weak visual understanding and logical errors. For
                            instance, in the above example, GPT-5-Thinking ignores real-world 3D object sizes and
                            considers only pixel-wise dimensions, incorrectly concluding the coffee table is six times
                            shorter than the sofa. These methods are also data-hungry, requiring extensive supervision.
                        </p>

                        <p>
                            Another line of work uses LLMs for program synthesis with vision specialists. Examples
                            include VADAR <a href="#vadar">[4]</a>, VisProg <a href="#visprog">[5]</a>, and ViperGPT <a
                                href="#vipergpt">[6]</a>. These training-free approaches
                            rely on proprietary LLMs and pre-trained specialists that are poorly aligned for visual and
                            spatial reasoning.
                        </p>

                        <h3>The VALOR Framework</h3>

                        <p>
                            We introduce VALOR, a scalable; annotation-free training framework that tackles spatial
                            reasoning from images by combining LLM-powered reasoning with specialized tool use. VALOR
                            employs an LLM to generate plans and executable programs and invokes vision specialists for
                            execution. Both the reasoning and the vision grounding model are tuned for the task via a
                            label-free training paradigm. This is achieved by leveraging multimodal verifiers that
                            critique model outputs. Their feedback serves as a learning signal to improve both
                            components, the LLM responsible for logic and the vision specialists responsible for
                            grounding. We name our approach VALOR as it integrates Verifiers for Annotation-free LOgic
                            and Reasoning.
                        </p>
                        <figure class="image is-fullwidth" style="margin: 1rem 0;">
                            <img src="./static/images/valor.png" alt="VALOR framework overview">
                            <!-- optional caption -->
                            <figcaption class="has-text-centered is-size-7" style="margin-top: 0.5rem;">
                                Figure 3: VALOR is a training framework for visual reasoning, tackling spatial reasoning
                                in both 2D and 3D. During training, LLM verifiers are used to improve reasoning via RL
                                while VLM verifiers serve as critics to tune vision grounding models
                                via SFT.
                            </figcaption>
                        </figure>

                        <p>
                            <strong>Plan and Code Generation.</strong> Given a query, the LLM generates a natural
                            language plan followed by a corresponding program in Python. Available to the LLM are the
                            APIs of three function calls:
                        </p>

                        <ul>
                            <li>
                                <code>GD_DETECT</code>, returns the bounding box of all object instances specified by
                                the noun description —
                                e.g., <code>GD_DETECT("CAR")</code> using a GroundingDINO model <a href="#gd">[7]</a>.
                            </li>
                            <li>
                                <code>DEPTH</code>, returns the depth of a pixel in the image —
                                <code>DEPTH(IMAGE, X, Y)</code> using MoGe2 <a href="#moge2">[8]</a>.
                            </li>
                            <li>
                                <code>VQA</code>, returns an object’s attribute (e.g., color) from the input image crop
                                around the object —
                                e.g., <code>VQA(IMAGE_CROP, "WHAT IS THE COLOR OF THE OBJECT IN THE IMAGE?")</code>
                                using GPT-5-mini <a href="#gpt5">[9]</a>.
                            </li>
                        </ul>

                        <h4>Improving Reasoning with LLM Verifiers</h4>
                        <p>
                            VALOR leverages LLM verifiers as a reward signal to improve reasoning via reinforcement
                            learning. LLM verifiers critique model outputs across a rubric of six criteria targeting
                            specfic aspects of spatial reasoning.
                        </p>
                        <figure class="image is-fullwidth" style="margin: 1rem 0;">
                            <video id="teaser" autoplay muted loop playsinline height="80%">
                                <source
                                    src="https://github.com/glab-caltech/valor/raw/refs/heads/main/static/videos/llm_verifier.mp4"
                                    type="video/mp4">
                            </video>
                            <!-- optional caption -->
                            <figcaption class="has-text-centered is-size-7" style="margin-top: 0.5rem;">
                                Video 1: VALOR uses LLM verifiers to improve reasoning via reinforcement learning.
                                Our reward consists of six binary components that target specific aspects
                                of spatial reasoning.
                            </figcaption>
                        </figure>

                        <p>
                            Our reward is composed of six binary rewards:
                        </p>

                        <ul>
                            <li>
                                <strong>Format Reward ($r_{\mathrm{fmt}}$):</strong> Ensures model outputs are properly
                                formatted. Format reward is 1 if the model output contains the proper
                                <code>&lt;plan&gt;...&lt;\plan&gt;</code> and
                                <code>&lt;answer&gt;...&lt;\answer&gt;</code> tags, and 0 otherwise.
                            </li>
                            <li>
                                <strong>Syntax Reward ($r_{\mathrm{sn}}$):</strong> Evaluates if the predicted program
                                executes without
                                Python errors. Syntax reward is 1 if the program executes properly with placeholder
                                variables, 0 otherwise.
                            </li>
                            <li>
                                <strong>Spatial Reward ($r_{\mathrm{sp}}$):</strong> An LLM verifies that the predicted
                                plan addresses all
                                spatial relationships in the query (above, behind, left of, etc.). The verifier returns
                                1 if all spatial relationships are addressed, 0 otherwise.
                            </li>
                            <li>
                                <strong>Attribute Reward ($r_{\mathrm{att}}$):</strong> An LLM verifier assesses whether
                                the plan specifies
                                explicitly and correctly how to compute all relevant attributes (height, color, etc.) in
                                the query. The LLM returns 1 if all attributes are computed correctly, otherwise 0.
                            </li>
                            <li>
                                <strong>Logic Reward ($r_{\mathrm{log}}$):</strong> An LLM verifier is given the query
                                and predicted plan, it
                                returns 1 if it considers the plan reasonable and coherent for the given query, 0
                                otherwise.
                            </li>
                            <li>
                                <strong>Adherence Reward ($r_{\mathrm{ad}}$):</strong> The predicted plan and code are
                                given to an LLM
                                verifier, which returns 1 if the code faithfully implements the plan without deviations,
                                0 otherwise.
                            </li>
                        </ul>

                        <p>
                            Our final reward is:
                        </p>

                        \[
                        R(q,p,c) = r_{\mathrm{fmt}}(p,c) \cdot \big [
                        \lambda_{\mathrm{sn}}\, r_{\mathrm{sn}}(c)
                        + \lambda_{\mathrm{log}}\, r_{\mathrm{log}}(q,p)
                        + \lambda_{\mathrm{att}}\, r_{\mathrm{att}}(q,p)
                        + \lambda_{\mathrm{sp}}\, r_{\mathrm{sp}}(q,p)
                        + \lambda_{\mathrm{ad}}\, r_{\mathrm{ad}}(p,c)\big ]
                        \]

                        <p>The format reward $r_{\text{fmt}}$ acts as a hard constraint and is applied as a multiplier,
                            while the
                            weighted sum of the remaining rewards evaluates content quality. All $r_k ∈ \{0, 1\}$ and
                            $\sum_k \lambda_k = 1.0$.
                        </p>


                        <h4>Improving Visual Grounding with VLM Verifiers</h4>

                        <p>
                            In addition to logic, visual reasoning relies on accurate grounding. Modern detectors like
                            GroundingDINO, trained on web data, are error-prone and struggle to generalize beyond their
                            training domains. Fine-tuning with domain-specific labels can mitigate these issues, but
                            collecting such annotations is labor intensive. We propose an alternative: improving visual
                            grounding through VLM verifiers. Vision specialists cast predictions, VLM verifiers evaluate
                            them, and the feedback augments their training set. This approach requires no manual
                            annotations and scales across domains without additional labels.
                        </p>

                        <figure class="image is-fullwidth" style="margin: 1rem 0;">
                            <video id="teaser" autoplay muted loop playsinline height="80%">
                                <source
                                    src="https://github.com/glab-caltech/valor/raw/refs/heads/main/static/videos/vlm_verifier.mp4"
                                    type="video/mp4">
                            </video>
                            <!-- optional caption -->
                            <figcaption class="has-text-centered is-size-7" style="margin-top: 0.5rem;">
                                Video 2: TODO: replace with video.
                            </figcaption>
                        </figure>

                        <p>
                            Our approach for verifier-improved visual grounding relies on image-query pairs $\{(I_j,
                            q_j)\}^M_{j=1}$. For each query $q_j$, our LLM reasoning model generates a plan and code,
                            $(p_j, c_j)$. From code $c_j$, we parse all grounding queries – e.g.,
                            <code>GD_DETECT(“HELMET”)</code> – and execute them with a pre-trained detector. To ensure
                            high recall, we lower the detector’s confidence threshold. This leads to overprediction,
                            which we validate with a frozen VLM verifier in three steps:
                        </p>

                        <ol>
                            <li>
                                <strong>Coarse Filtering:</strong> The input image with all candidate detections is
                                passed to the verifier, which is prompted to discard all boxes where the object does not
                                match the box label.
                            </li>
                            <li>
                                <strong>Per-crop Object Check:</strong> The verifier is given a cropped image of each
                                remaining box and asked to verify if the object visible in the crop matches the
                                predicted label. All incorrect boxes are discarded.
                            </li>
                            <li>
                                <strong>Deduplication:</strong> The input image with all remaining detections is shown
                                to the verifier, which is tasked with discarding all duplicate predictions. For each
                                set of duplicates, the VLM is asked to retain the most correct box.
                            </li>
                        </ol>

                        <p>
                            Confirmed detections form a new training set, which we use to fine-tune a pre-trained
                            GroundingDINO detector.
                        </p>

                        <h4>Inference</h4>

                        <p>
                            The predicted Python programs, that invoke our vision-specialist APIs, are executed to
                            produce answers to visual reasoning queries.
                        </p>

                        <figure class="image is-fullwidth" style="margin: 1rem 0;">
                            <video id="teaser" autoplay muted loop playsinline height="80%">
                                <source
                                    src="https://github.com/glab-caltech/valor/raw/refs/heads/main/static/videos/inference.mp4"
                                    type="video/mp4">
                            </video>
                            <!-- optional caption -->
                            <figcaption class="has-text-centered is-size-7" style="margin-top: 0.5rem;">
                                Video 4: TODO: replace with video.
                            </figcaption>
                        </figure>


                        <h4>Aside: can trained models ever outperform the verifiers?</h4>
                        <figure class="image is-fullwidth" style="margin: 1rem 0;">
                            <img src="./static/images/valor_vs_gpt5.png"
                                alt="Despite being a good verifier for visual grounding, GPT5 struggles with visual grounding.">
                            <!-- optional caption -->
                            <figcaption class="has-text-centered is-size-7" style="margin-top: 0.5rem;">
                                Figure 4: Despite being a powerful VLM verifier for visual grounding, GPT-5-mini
                                struggles
                                with generating boxes for object grounding.
                            </figcaption>
                        </figure>

                        <p>
                            A natural question to ask is whether VALOR can ever outperform the veriifers it uses
                            during
                            training. To this end, we first note that VALOR uses multimodal verifiers to
                            <em>select</em>
                            and <em>critique</em> data, not <em>generate</em> it. Thus, VALOR is not bound by the
                            <em>generation</em> abilities of an LLM/VLM, but rather it's <em>verification</em>
                            ability.
                            This distinction is important as we find there are tasks where VLMs are better verifiers
                            than generators. For a concrete example, in VALOR, we use GPT-5-mini as our VLM verifier
                            for
                            improving the visual grounding module. Although highly effective at evaluating object
                            detections, we observe that it often struggles generating bounding boxes itself. In the
                            figure above, we find that GPT-5-mini frequently outputs misaligned or overly large
                            boxes,
                            failing to localize objects that VALOR (trained with GPT-5-mini as a verifier) correctly
                            detects. We find that a VLM canprovide reliable binary judgements about correctness even
                            when its own grounding predictions are imperfect.
                        </p>

                        <h3>Evaluation</h3>

                        <p>
                            We evaluated a series of open-source models, as well as VALOR, across a wide range of
                            spatial reasoning benchmarks. Each LLM was used language-only, and was prompted to
                            generate Python programs that can invoke an API of vision specialist models (detection,
                            depth estimation, VQA), as described above. We then executed the generated programs to
                            determine accuracy on each of the bechmarks.
                        </p>


                        <h4>How do open-source models perform?</h4>
                        <figure class="image is-fullwidth" style="margin: 1rem 0;">
                            <img src="./static/images/os_bar_plot.png"
                                alt="open-source model performance on spatial reasoning benchmarks">
                            <!-- optional caption -->
                            <figcaption class="has-text-centered is-size-7" style="margin-top: 0.5rem;">
                                Figure 5: Open-source language-only models generate Python programs with
                                access to an API of vision specialist models. We execute the programs and report
                                accuracy.
                            </figcaption>
                        </figure>

                        <p>
                            Among the open-source models we evaluated (Llama3.2-11B, Gemma3-12B,and Qwen3-8B), Qwen3
                            consistently performed the best. We found that despite using the
                            instruction-tuned variants, Gemma3 and Llama3.2 routinely ignored our system prompts.
                            For
                            example, both models would frequently overwrite the input image path, define
                            "placeholder"
                            values, or argue the query was impossible and refuse to answer altogether. In contrast,
                            Qwen3 consistently produced reasonable programs, but incorrectly handled nuanced details
                            in
                            the query and failed to use tools effectively. We felt these were issues that could be
                            addressed via post-training, so decided to build on the capable Qwen3 model for VALOR.
                        </p>

                        <h4>Qwen3 vs VALOR-RL: Training with verifiers improves model reasoning.</h4>
                        <figure class="image is-fullwidth" style="margin: 1rem 0;">
                            <img src="./static/images/os_valor_rl.png"
                                alt="Training with verifiers improves model reasoning">
                            <!-- optional caption -->
                            <figcaption class="has-text-centered is-size-7" style="margin-top: 0.5rem;">
                                Figure 6: Verifier-guided training improves model reasoning and tool use, particularly
                                on reasoning-heavy benchmark Omni3D-Bench.
                            </figcaption>
                        </figure>
                        <p>
                            We compare VALOR-RL with Qwen3 to isolate the impact of verifier-improved reasoning.
                            VALOR-RL uses a verifier-trained Qwen3 model with the same vision specialist models.
                            Thus
                            any improvements from Qwen3 to VALOR-RL stem from our LLM-verifier guided training.
                            VALOR-RL
                            shows gains over Qwen3: +3.4% on BLINK, +2.1% on VSR, and +1.3% on RoboSpatial. Most
                            notably, VALOR-RL greatly improves on Omni3D-Bench (+6.4%), our most reasoning-intensive
                            benchmark. On counting tasks TallyQA and CountBenchQA, reasoning is less critical, and
                            VALOR-RL matches Qwen3.
                        </p>
                        <h4>VALOR-RL vs VALOR: Training with verifiers improves visual grounding.</h4>
                        <figure class="image is-fullwidth" style="margin: 1rem 0;">
                            <img src="./static/images/valor_vs_valor_rl.png"
                                alt="Training with verifiers improves visual grounding">
                            <!-- optional caption -->
                            <figcaption class="has-text-centered is-size-7" style="margin-top: 0.5rem;">
                                Figure 7: Verifier-guided training improves visual grounding, yielding improved
                                performance across benchmarks.
                            </figcaption>
                        </figure>
                        <p>
                            In the above plot we compare VALOR, our final method, to VALOR-RL. The two variants
                            execute
                            identical programs, though VALOR uses the verifier-improved visual grounding module.
                            VALOR
                            yields strong gains across the board, particularly on grounding-focused benchmarks:
                            +8.3%
                            on CountBenchQA, +7.7% on RoboSpatial, and +5.3% on VSR. Improvements on Omni3D-Bench
                            are
                            smaller, as complex queries make reasoning the main challenge for smaller LLMs. Notably,
                            improving visual grounding for spatial reasoning does not harm general object detection;
                            our
                            training slightly boosts performance on the COCO validation set: 48.4% to
                            48.7% mAP.
                        </p>
                        <h4>VALOR Outputs</h4>
                        <p>
                            TODO: Video of examples.
                        </p>
                        <h4>Conclusion</h4>
                        <p>
                            We introduce VALOR, an annotation-free training paradigm for visual reasoning that
                            leverages multimodal verifiers to improve LLM reasoning and visual grounding, leading to
                            significant improvements on a wide range of spatial reasoning benchmarks. We find that
                            VLMs/LLMs are increasingly capable verifiers, not merely generators. In fact, we find there
                            are tasks where they are excellent verifiers but not great generators (e.g. object
                            detection). This suggests an alternative method to improving reasoning in the visual domain:
                            leveraging the multimodal verification capabilities of these models to enable training in
                            domains where ground truth is unavailable.
                        </p>

                        <strong>Acknowledgements</strong>
                        <p>
                            We would like to thank ...
                        </p>
                        <!-- 
                        <strong>Recommended Reading</strong>
                        <ol>
                            <li><a href="https://visually-grounded-rl.github.io/">Visually Grounded Reinforcement
                                    Learning</a></li>
                            <li><a href="https://grounded-reasoning.github.io/">Teaching MLLMs to Think with
                                    Images</a>
                            </li>
                            <li><a href="https://openai.com/index/thinking-with-images/">Thinking with Images</a>
                            </li>
                            <li><a href="https://glab-caltech.github.io/vadar/">Visual Agentic AI for Spatial
                                    Reasoning
                                    with a Dynamic API</a></li>
                        </ol> -->

                        <strong>References</strong>
                        <ol>
                            <li id="thinking-with-images">
                                OpenAI. <em>Thinking with Images</em>. URL: <a
                                    href="https://openai.com/index/thinking-with-images/">https://openai.com/index/thinking-with-images/</a>
                            </li>
                            <li id="grit">
                                Yue Fan, Xuehai He, Diji Yang, Kaizhi Zheng, Ching-Chen Kuo, Yuting Zheng, Sravana
                                Jyothi Narayanaraju, Xinze Guan, and Xin Eric Wang.
                                <em>Grit: Teaching mllms to think with images</em>. in <em>NeurIPS</em>, 2025.
                            </li>
                            <li id="vigorl">
                                Gabriel Sarch, Snigdha Saha, Naitik Khandelwal, Ayush Jain, Michael J Tarr, Aviral
                                Kumar, and Katerina Fragkiadaki.
                                <em>Grounded reinforcement learning for visual reasoning</em>. in <em>NeurIPS</em>,
                                2025.
                            </li>

                            <li id="vadar">
                                Damiano Marsili, Rohun Agrawal, Yisong Yue, and Georgia Gkioxari.
                                <em>Visual agentic ai for spatial reasoning with a dynamic api</em>. in <em>CVPR</em>,
                                2025.
                            </li>

                            <li id="visprog">
                                Tanmay Gupta and Aniruddha Kembhavi.
                                <em>Visual programming: Compositional visual reasoning without training</em>. in
                                <em>CVPR</em>,
                                2023.
                            </li>

                            <li id="vipergpt">
                                Didac Suris, Sachit Menon, and Carl Vondrick.
                                <em>Vipergpt: Visual inference via python execution for reasoning</em>. in
                                <em>ICCV</em>,
                                2023.
                            </li>


                            <li id="gd">
                                Shilong Liu, Zhaoyang Zeng, Tianhe Ren, Feng Li, Hao Zhang, Jie Yang, Chunyuan Li,
                                Jianwei Yang, Hang Su, Jun Zhu, et al
                                <em>Grounding dino: Marrying dino with grounded pre-training for
                                    open-set object detection</em>. arXiv preprint arXiv:2303.05499, 2023b.
                            </li>

                            <li id="moge2">
                                Ruicheng Wang, Sicheng Xu, Yue Dong, Yu Deng, Jianfeng Xiang, Zelong Lv, Guangzhong Sun,
                                Xin Tong, and Jiaolong Yang.
                                <em> Moge-2: Accurate monocular geometry with metric scale and
                                    sharp details.</em>. in
                                <em>CVPR</em>,
                                2025.
                            </li>
                            <li id="gpt5">
                                OpenAI. <em>GPT-5-mini</em>. URL: <a
                                    href="https://openai.com/index/introducing-gpt-5/">https://openai.com/index/introducing-gpt-5/</a>
                            </li>

                        </ol>

                    </div>
                </div>
            </div>
        </div>
    </section>


    <section class="section" id="BibTeX">
        <div class="container is-max-desktop content">
            <h2 class="title">BibTeX</h2>
            <pre>
<code>TODO.</code></pre>
        </div>
    </section>

    <div id="modal" class="modal">
        <div class="modal-background"></div>

        <div class="modal-content">
            <div id='plot-loading-div' class="box">
                Loading...
            </div>
            <div id='plot-div'></div>
        </div>

        <button class="modal-close is-large" aria-label="close"></button>
    </div>

</body>

</html>